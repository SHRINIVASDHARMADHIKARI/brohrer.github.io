<!DOCTYPE html>
<html>

  <script type="text/javascript">var blog_title = "How to Convert a Picture to Numbers";</script>
  <script type="text/javascript">var publication_date = "November 17, 2019";</script>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <base target="_blank">
    <script type="text/javascript" src="javascripts/blog_head.js"></script>
  </head>
  <body>
    <script type="text/javascript" src="javascripts/blog_header.js"></script>
    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
      
        <p>
          The experience of seeing the world around us is all but impossible
          to capture in words,
          from the careful steps
          of a marching ant,
          to the works of Pablo Picasso and Beatrix Potter,
          to a solitary oak tree, twisted and dignified.
          It's ridiculous to think that we could ever reduce it all to
          ones and zeros. Except that we have. In fact,
          our images are so realistic now that we go to pains to 
          re-introduce artifacts, like the
          washed out colors of Polaroids or the scratches on celluloid.
        </p>
        <p>
          This may be a blow to romamticism, but it's great luck for
          machine learning practitioners. Reducing images to numbers
          makes them amenable to computation.
        </p>
        <h3>Color perception</h3>
        <p>
          Color fascinates me because it is less about physics than it
          is about the physiology and psychology of human perception.
          All our standards are determined by
          what humans perceive. The range that needs to
          be covered, the number of channels necessary to
          represent a color, the resolution with which a color must be
          specified, and hence the information density and
          storage requirements, all depend on human retinas and
          visual cortices.
        </p>
        <p>
          It also means that, as with everything
          human, there is a great deal of variability.
          There are deficiencies like
          <a href="https://en.m.wikipedia.org/wiki/Color_blindness">
            color blindness</a>
          (I myself experience
          <a href="https://en.m.wikipedia.org/wiki/Color_blindness#Deuteranomaly">
            deuteranomaly</a>
          , a type of
          red-green colorblindness) and there are those with unusual
          abilities, like
          <a href="https://en.m.wikipedia.org/wiki/Tetrachromacy">
            tetrachromats</a>
          , who have not three types of
          color receptors, but four, and can distinguish colors
          that the rest of us can’t.
          Because of this, keep in mind that all of the statements
          we can make about perception are generalizations only,
          and there will be individual differences.
        </p>
        <p>
          Although photons vibrate at all frequencies,
          we have three distinct types of color-sensing cones,
          each with its characteristic frequency response.
          That means that a combination of three light sources
          with carefully chosen colors
          in carefully chosen intensities can make us experience
          any color that we're
          capable of seeing in the natural world.
        </p>
        <h3>Making color</h3>
        <p>
          In computer screens, this is done with a red, a green, and a blue
          light, often light-emitting diodes (LEDs).
        </p>
        <p>
          <img title="A pixel composed of red, green, and blue LEDs"
            src="images/image_processing/rgb_pixel.png"
            alt="A pixel composed of red, green, and blue LEDs"
            style="height: 300px;">
        </p>
        <p>
          In practice the red, green, and blue LEDs in a computer
          screen can’t represent all the colors we can see. To make
          a colored LED, a chemical is introduced that fluoresces
          at about the right color. These are close to ideal
          red, blue, and green but they aren’t perfect. For this
          reason there is a bit of a gap between the range of
          colors that you can see in the real world (the
          <a href="https://en.wikipedia.org/wiki/Gamut">gamut</a>
          ) and what you can see on a computer screen.
        </p>
        <p>
          As a side note, lasers are capable of
          producing color much closer to the ideal. Commercially available laser
          projection systems cover much more of the human perceivable
          gamut, and laser micro arrays for computer screens are
          a current topic of research and development.
        </p>
        <p>
          <img title="An array of pixels composed of red, green, and blue LEDs"
            src="images/image_processing/rgb_pixel_array.png"
            alt="An array of pixels composed of red, green, and blue LEDs"
            style="height: 300px;">
        </p>
        <h3>Turning color into numbers</h3>
        <p>
          Each pixel in a screen is a triplet of a red, a green, and a blue
          light source, but when you look at them from far enough away
          they are too small for your eye to distinguish and they look
          like a single small patch of color. One way to determine which color
          is produced is to specify the intensity levels of each of the
          light sources. Since the
          <a href="https://en.wikipedia.org/wiki/Just-noticeable_difference">
            just noticeable difference (JND)</a> 
          in human perception of color intensity is somewhere
          around one part in a hundred, using 256 discrete levels gives
          enough fine-grained control that smooth color gradients still
          look smooth.
        </p>
        <p>
          256 intensity levels can be represented with 8 bits or 1 byte. It can
          also be represented with two hexadecimal numbers, between 0x00
          for zero brightness
          and 0xff for maximum brightness.
          Specifying the intensity of three colors takes triple that: 
          6 hexidecimal numbers (24 bits or 3 bytes).
          The hex representation gives a concise way to call out a
          red-green-blue color. The first two digits show the red level,
          the second two correspond to the green level, and the
          third pair correponds to the blue level. Here are a few extremes.
        </p>
        <p>
          <img title="Hex codes of a few colors"
            src="images/image_processing/color_hex.png"
            alt="Hex codes of a few colors"
            style="height: 600px;">
        </p>
        <p>
          There are a lot more useful
          <a href="https://www.color-hex.com/">color hex codes here</a>.
          For convenience and code readability, colors can also be represented
          as triples like (255, 255, 255) for white or (0, 255, 0) for green.
        </p>
        <h3>Building an image from pixels</h3>
        <p>
          To recreate an entire image,
          computers use their reliable trick of simply chopping it up into
          small pieces. To make high quality images,
          it's necessary to make the pieces are so small that the
          human eye has trouble seeing them individually. 
        </p>
        <p>
          <img title="Progressive zoom showing pixels in an image"
            src="images/image_processing/zoom_pixels.png"
            alt="Progressive zoom showing pixels in an image"
            style="height: 250px;">
          Image credit: Diane Rohrer
        </p>
        <p>
          The color of each pixel can be represented as a 6-digit hex
          number or a triple of decimal numbers ranging from 0 to 255.
          During image processing it's customary to do the latter.
          For convenience, the red, green, and blue pixel values are
          separated out into their own arrays.
        </p>

        <p>
          <img title="Arrays of pixel colors broken down into red, green, and blue"
            src="images/image_processing/rgb_arrays.png"
            alt="Arrays of pixel colors broken down into red, green, and blue"
            style="height: 700px;">
        </p>

        <h3>Reading images into Python code</h3>
        <p>
          A reliable way to read images into Python is with
          <a href="https://pillow.readthedocs.io/en/stable/">
            Pillow</a> an actively maintained fork of the
          classic Python Image Library or PIL, and Numpy.
        </p>
        <p>
<pre><code>import numpy as np
from PIL import Image
img = np.asarray(Image.open("image_filename.jpg"))</code></pre>
        </p>
        <p>
          When reading in a color image, the resulting object
          <code>img</code> is a three-dimensional
          Numpy array. The data type is often
          <a href="https://docs.scipy.org/doc/numpy/user/basics.types.html">
            <code>numpy.uint8</code></a>, which is a natural and efficient
          way to represent color levels between 0 and 255. I haven't
          been able to determine that this is always the case, so it's
          safest to confirm for the images in your dataset before you
          start operating on them.
        </p>
        <p>
          In order to facilitate calculations, I find it most convenient
          to convert the image values to floats between 0 and 1.
          In python3, the easiest way to do this is to divide by 255:
          <code>img *= 1/255</code>
        </p>
        <p>
          It's helpful to remember that when images are stored and
          transmitted, they can be represented using
          a dizzying variety of formats. Parsing these is a separate
          effort. We'll let Pillow do all the heavy lifting in this
          department, and rely on <code>open()</code> and
          <code>asarray()</code> to do all those conversions for us.
          I still haven't found a way around having to verify your
          pixels' range and data types without checking, but I'll keep
          my eyes open.
        </p>
        <p>
          Now we have all the image information in a compact collection
          of numbers

        <script type="text/javascript" src="javascripts/blog_signature.js"></script>
      </section>
    </div>
    <script type="text/javascript" src="javascripts/blog_footer.js"></script>
    <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
      try {
        var pageTracker = _gat._getTracker("UA-10180621-3");
      pageTracker._trackPageview();
      } catch(err) {}
    </script>
  </body>
</html>
