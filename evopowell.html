<!DOCTYPE html>
<html>

  <script type="text/javascript">var blog_title = "Evolutionary Powell's method";</script>
  <script type="text/javascript">var publication_date = "December 15, 2019";</script>
  <head>
    <link rel="icon" href="images/ml_logo.png">
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <base target="_blank">
    <script type="text/javascript" src="javascripts/blog_head.js"></script>
  </head>
  <body>
    <script type="text/javascript" src="javascripts/blog_header.js"></script>
    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>A discrete optimizer for hyperparameter optimization</h3>

        <p style="text-align:center;">
          <img
            alt="Animation of Evolutionary Powell's method finding an optimum"
            src="https://gitlab.com/brohrer/ponderosa/raw/master/ponderosa/landing_page_demo.gif"
            style="height: 350px;">
        </p>
        <p>
          I built Evolutionary Powell's method as a hyperparameter optimization
          algorithm as part of
          <a href="https://end-to-end-machine-learning.teachable.com/p/314-neural-network-optimization/">
            Course 314</a>. It is inspired by the original
          <a href="https://en.wikipedia.org/wiki/Powell%27s_method">
            Powell's method</a>, but has a stochastic element
          inspired by
          <a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">
            evolutionary approaches</a>.
          A Python implementation of the algorithm is available in
          <a href="https://gitlab.com/brohrer/ponderosa">
            the Ponderosa optimization package</a> under an
          <a href="https://choosealicense.com/licenses/mit/">
            MIT Open Source License</a>.
        </p>

      <h3>Overview</h3>
        <p>
          Evolutionary Powell's method tries to find the global minimum
          of a loss function (cost function, error function) over a
          discrete space, meaning that each variable can only take on
          a finite number of unique values. It has a few main steps, which
          we'll illustrate by walking through the example in the animation
          above.
        </p>
        <p>
          The loss function we'll be working with is a two-dimensional
          <a href="https://en.wikipedia.org/wiki/Sinc_function">
            <em>sinc</em></a> function, which has a single pronounced
          peak and several shoulder peaks and dips surrounding it.
          The code for creating it and running the rest of this example
          can be found in
          <a href="https://gitlab.com/brohrer/ponderosa/blob/master/ponderosa/demo.py">
            this GitLab project</a>.
        </p>
        <p>
          The optimizer actually tries to find
          the lowest point in the the negative of this function &mdash;
          the global minimum &mdash; but for ease of visualization everything
          is flipped upside down before plotting. Both x and y are allowed
          to take on 10 distinct values between 0 and 3. The method can
          be applied to spaces of any number of dimensions, but a
          two-dimensional space gives the richest visualization.
        </p>
        <p>
          There are a few main steps to Evolutionary Powell's method.
          <ol>
            <li>
              Randomly select a few points and evaluate them.
            </li>
            <li>
              Make a list of the hyperparameters in random order.
            </li>
            <li>
              Choose a small number of previously-evaluated points
              to be parents. This will be
              a random choice, but weighted by performance so that points
              with the lowest error are more likely to be chosen.
            </li>
            <li>
              For the first parent added, start at the top of the list
              of hyperparameters. Look for a small number of unevaluated
              child points along that line &mdash; points that are identical 
              to the parent, differing only in the value of that particular
              hyperparameter. If no children can be found, check the next
              hyperparameter, until they have all been attempted.
            </li>
            <li>
              For each subsequent parent added, repeat the process, but
              shift the list of hyperparameters by one so that a different
              hyperparameter is attemped first.
            </li>
            <li>
              Evaluate all the children found and start again at step 3.
            </li>
            <li>
              When no children can be found for any of the parents, the
              algorithm terminates.
            </li>
          </ol>
        </p>

        <h3>Powell's method</h3>
        <p>
          This is a little like Powell's method where, starting from a single
          point, it finds the minimum along one dimension at a time,
          and the new best guess is moved iteratively to the new location
          of the minimum. (
          <a href="https://en.wikipedia.org/wiki/Powell%27s_method">
            There's a bit more to it than that</a>, but those
          are the relevant parts.)
        </p>
        <p>
          Powell's method typically assumes smooth, continuously varying
          functions, and seeks just to find the local minimum, so it
          alone isn't a good fit for hyperparameter optimization
          where we are looking for a global minimum in a discrete space.
          But it at least is nice in that it doesn't require differentiation
          and it manages a large number of dimensions gracefully.
        </p>

        <h3>Evolutionary algorithms</h3>
        <p>
          In high-dimensional discrete spaces, evolutionary algorithms have
          proven themselves useful. They don't have to
          assume anything about the
          smoothness of a function, and they can be used as
          <a href="https://en.wikipedia.org/wiki/Anytime_algorithm">
            an anytime algorithm</a>, meaning that you can interrupt the
          algorithm at any time and have a workable best-so-far answer
          to work with.
        </p>
        <p>
          Evolutionary algorithms are especially useful when there are
          far more points in the space than could ever be exhaustively
          explored. They make the assumption that the lowest-error points
          will have at least some traits in common with other
          low-error points. In our case, that assumption is expressed as
          "successful points will have <em>some</em> identical hyperparameter
          values". Choosing the most-successful-so-far points as parents,
          and varying one of their hyperparameter values to create children,
          is a way to exploit this assumption. This is also where
          the similarity to Powell's method comes from. Exploring
          one dimension at a time, starting from a high performing point,
          looks a lot like the iterative one-dimensional optimization that
          Powell's method performs.
        </p>



        <script type="text/javascript" src="javascripts/blog_signature.js"></script>
      </section>
    </div>
    <script type="text/javascript" src="javascripts/blog_footer.js"></script>
    <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
      try {
        var pageTracker = _gat._getTracker("UA-10180621-3");
      pageTracker._trackPageview();
      } catch(err) {}
    </script>
  </body>
</html>
